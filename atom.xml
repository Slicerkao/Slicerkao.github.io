<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tr{Ceng Lou}]]></title>
  <link href="http://slicerkao.github.io/atom.xml" rel="self"/>
  <link href="http://slicerkao.github.io/"/>
  <updated>2019-11-09T12:25:54-05:00</updated>
  <id>http://slicerkao.github.io/</id>
  <author>
    <name><![CDATA[Ping Sheng Kao]]></name>
    <email><![CDATA[samk840125@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Slide share: Quantum 101]]></title>
    <link href="http://slicerkao.github.io/blog/2019/11/08/slide-share-quantum-101/"/>
    <updated>2019-11-08T13:36:58-05:00</updated>
    <id>http://slicerkao.github.io/blog/2019/11/08/slide-share-quantum-101</id>
    <content type="html"><![CDATA[<p>It’s always fun to organize knowledge and distill it to a simpler form. I’m happy to get a chance to share my understanding of quantum computation with a group of talented engineers, and I hope to deliver concepts with as much intuitive examples as possible.</p>

<p>In this post, I share two of my quantum computation slides. The first one is quantum computation 101, which established foundation and intuitions of quantum computation. The second goes on to explore topics of machine learning and its’ interplay with quantum computation.</p>

<p>Btw, some of the ‘puns’ or acronym used in the slides are purely mandarine, only to catch the attention of my chinese-speaking audiences.
<!--more--></p>

<p style="text-align: center;">
<iframe src="https://www.slideshare.net/slideshow/embed_code/191851193 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen=""></iframe> 
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Study Note: Product distribution(quantum state) increase and split the divergence ?]]></title>
    <link href="http://slicerkao.github.io/blog/2018/07/22/study4/"/>
    <updated>2018-07-22T10:41:58+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/07/22/study4</id>
    <content type="html"><![CDATA[<p>Given a divergence measure $d(P, Q)$ and two distribution $P$ $Q$, what is the effect to the divergence $d(P^m, Q^m)$ as $m$ increases ? Intuitively, the divergence shall increase since product distribution emphasize the difference of the distribution. In <a href="https://arxiv.org/abs/1712.04086">this work by Zinan et.al </a>, the upper and lower bound of $d(P^m, Q^m)$ is given for the divergence measure :</p>

<p>\begin{eqnarray}
&amp;&amp; d _{p}(P,Q) = \mathop{\sup} _{S} [P(S) - Q(S)]
\end{eqnarray}</p>

<p>This measure is closely related to the optimal successful detection probability $P _{\rm opt}$ in the binary hypothesis testing :</p>

<p>\begin{eqnarray}
&amp;&amp; P _{\rm opt} = \mathop{\sup} _{S} [\pi _1 P(S) + \pi _2 (1 - Q(S))] = \pi _2 + d _{p}(\pi _1 P, \pi _2 Q) = \frac{1}{2} + d _{\rm TV}(\pi _1 P, \pi _2 Q)<br />
\end{eqnarray}</p>

<p>where $\pi _1, \pi _2$ are the prior distribution of the hypothesis, and $d _{\rm TV}(P, Q)$ is the total variance distance :</p>

<p>\begin{eqnarray}
&amp;&amp; d _{TV}(P,Q) = \frac{1}{2} \sum _{\omega \in \Omega} |P(\omega) - Q(\omega)|
\end{eqnarray}</p>

<p>What makes the work by Zinan et.al interesting is that if certain local property is satisfied for the distribution pair $(P, Q _1)$ but not for $(P, Q _2)$, and assume $d _p(P, Q _1) = d _p(P, Q _2)$, then the possible value region of $d _p(P^m, Q^m _1)$ splits away from that of $d _p(P^m, Q^m _2)$. This local property is called $(\epsilon, \delta)$ -mode collapse, defined as:</p>

<p>\begin{eqnarray}
&amp;&amp; P,Q \; {\rm has \; (\epsilon, \delta)- mode \; collapse \; :}\exists S \; {\rm s.t.} \; P(S) \geq \delta \; , Q(S) \leq \epsilon 
\end{eqnarray}</p>

<p><img src="http://slicerkao.github.io/images/ud.png" alt="alt text" /></p>

<p>Interestingly, similar result was given in the quantum literature by <a href="https://arxiv.org/pdf/1503.03048.pdf">Jonas
Maziero</a> for trace distance:</p>

<p>\begin{eqnarray}
&amp;&amp; d _1 (\rho _1, \rho _2) = \frac{1}{2} \Vert \rho _1 - \rho _2 \Vert _1 = {\rm Tr} (\sqrt{(\rho _1 - \rho _2)^{\ast} ( \rho _1 - \rho _2)})
\end{eqnarray}</p>

<p>So, I think it would be fun to dig into the quantum counterpart of Zinan’s work.</p>

<!--more-->

<h3 id="a-quantum-counterpart">A Quantum Counterpart</h3>

<p>In the quantum paradigm, the discrete probability distribution is replaced by the density matrix $\rho$, which is a positive semidefinite matrix with trace one. When the matrix is diagonal, it’s reduced back to the classical probability distribution.</p>

<p>For a classical distribution, calculating $P(S)$ for some region $S$ is
intuitive and straightforward: It’s the probability of observing events falling within the region $S$. More explicitly, :</p>

<p>\begin{eqnarray}
&amp;&amp; P(S) = \sum _{\omega \in S} P(\omega) \cdot 1 + \sum _{\omega \notin S}
P(\omega) \cdot 0 = \langle P, \mathbb{1} _{S} \rangle 
\end{eqnarray}</p>

<p>where $\mathbb{1} _S$ is a vector having one on the support $S$ and zero for the other components. This idea is generalized in the quantum domain by quantum measurement. To be more explicit, quantum measurement $\mu$ is a collection of positive semidefinite matrix, such that:</p>

<p>\begin{eqnarray}
&amp;&amp; \sum _{a \in {\rm some “event” support}} \mu (a) = \mathbb{1}
\end{eqnarray}</p>

<p>When measurement is performed on a quantum state $\rho$, event $a$ is selected with probability $\langle \mu (a), \rho \rangle = Tr(\mu (a) ^{\ast} \rho)$. Take the task of binary hypthesis testing as an example, the collection of measurement operator $[\mu (0) , \mu (1)]$ indicates the observation comes from $\rho _{P}$ or $\rho _{Q}$. The successful detection probability is:</p>

<p>\begin{eqnarray}
&amp;&amp; P _{\rm opt}=\pi _1 \langle \mu (0), \rho _{P}\rangle + \pi _2 \langle \mu (1), \rho _{Q}\rangle
\end{eqnarray}</p>

<p>which is further associated with the trace distance (reduce to $d _{TV}$ for
classical case), by the well known Holevo-Helstrom theorem:</p>

<p>\begin{eqnarray}
&amp;&amp; P _{\rm opt}= \pi _1 \langle \mu (0), \rho _{P}\rangle + \pi _2 \langle \mu (1), \rho _{Q}\rangle \leq \frac{1}{2} + \frac{1}{2}\Vert \pi _1 \rho _{P} - \pi _2 \rho _{Q} \Vert _1
\end{eqnarray}</p>

<p>where the equality is established by constructing projective measurement from the Jordan-Hahn decomposition:</p>

<p>\begin{eqnarray}
&amp;&amp; \pi _1 \rho _{P} - \pi _2 \rho _{Q} = M - N \; , \; \mu^{\ast} (0) = \Pi _{\rm im \; M} \; , \; \mu^{\ast} (1) = \mathbb{1} - \Pi _{\rm im \; M}
\end{eqnarray}</p>

<p>where $(M,N)$ are both positive semidefinite, and $\Pi _{\rm im \; M} $ is the projection operator onto the image of $M$.  This corresponds to the optimal $S$ given by $d _{p}(\pi _1 P, \pi _2 Q)$ in the case $\rho _{P} \; \rho _{Q}$ are both diagonal.</p>

<p>($\epsilon, \delta$)- mode collapse can be rephrased in a quantum manner. State pair $(\rho _{P}, \rho _{Q})$ has ($\epsilon, \delta$) -mode collapse if there exist a measurement operator $\mathbb{0} \leq \mu(a) \leq \mathbb{1}$ with $a \in \mathcal{A}$ for some collection, such that $\langle \mu (a), \rho _{P}\rangle \geq \delta$ and $\langle \mu (a), \rho _{Q} \rangle \leq \epsilon$.</p>

<h3 id="theorem-statements">Theorem statements</h3>

<p>Before heading toward some examples in the next section, we first state the theorem by Zinan et.al:</p>

<div class="theorem">
For all $ 0 \leq \tau \leq 1$ and positive integer $m$, given $d _p(P, Q) = \tau$, we have that:
\begin{eqnarray}
&amp;&amp; \mathop{\min} _{0 \leq \alpha \leq 1 - \tau} d _{p}(P _{\rm inner} (\alpha) ^m , Q _{\rm inner} (\alpha, \tau) ^m) \leq d _p (P ^m, Q ^m) \leq 1 - (1- \tau) ^m
\end{eqnarray}
where 
\begin{eqnarray}
&amp;&amp; P _{\rm inner} (\alpha) = [1 - \alpha, \alpha] \newline
&amp;&amp; Q _{\rm inner} (\alpha, \tau) = [1 - \alpha - \tau, \alpha + \tau]
\end{eqnarray}

Meanwhile, if $(P,Q)$ has $(\epsilon, \delta) -$ mode collapse, then given $d _p (P, Q) = \tau \leq \delta - \epsilon$, we have that:
\begin{eqnarray}

&amp;&amp; L _1(\epsilon, \delta, \tau, m)\leq d _p (P ^m, Q ^m) \leq 1 - (1- \tau) ^m 
\end{eqnarray}

with

\begin{eqnarray}
&amp;&amp; P _{\rm inner1}(\delta ,  \alpha) = [\delta, 1 - \alpha - \delta , \alpha]  \newline
&amp;&amp; Q _{\rm inner1}(\epsilon, \alpha, \tau) = [\epsilon, 1- \alpha -\tau -
\epsilon, \alpha + \tau] \newline
&amp;&amp; P _{\rm inner2} (\alpha) = [1 - \alpha, \alpha] \newline
&amp;&amp; Q _{\rm inner2} (\alpha, \tau) = [1 - \alpha - \tau, \alpha + \tau] \newline
&amp;&amp; L _1 (\epsilon , \delta, \tau, m) =  {\rm min} \big( \mathop{\min} _{0 \leq \alpha \leq 1 - \frac{\tau \delta}{\delta - \epsilon}} d _{p}(P _{\rm inner1} (\delta ,\alpha) ^m , Q _{\rm inner1} ( \epsilon, \alpha, \tau) ^m) , \mathop{\min} _{1 - \frac{\tau \delta}{\delta - \epsilon} \leq \alpha \leq 1- \tau} d _{p}(P _{\rm inner2} (\delta ,\alpha) ^m , Q _{\rm inner2} ( \epsilon, \alpha, \tau) ^m)\big) 
\end{eqnarray}

Finally, if $(P,Q)$ has no $(\epsilon, \delta) -$ mode collapse, then given $d _p(P,Q) = \tau$, we have three different cases :

(1  If $0 \leq \tau &lt; \delta - \epsilon$, then the upper and the lower bound follws the first unconstrained one.

(2  If $\delta + \epsilon \leq 1$ and $\delta -\epsilon \leq \tau \leq (\delta  -\epsilon)/(\delta + \epsilon)$ we have:

\begin{eqnarray}
&amp;&amp; U _1(\epsilon , \delta, \tau, m) \leq d _p(P ^m, Q ^m) \leq L _2(\tau, m)
\end{eqnarray}

with

\begin{eqnarray}
&amp;&amp; P _{\rm outer1} (\epsilon, \delta, \alpha, \beta , \tau) = [\frac{\alpha (\delta - \epsilon) - \epsilon \tau}{\alpha - \epsilon}, \frac{\alpha(\alpha + \tau - \delta)}{\alpha - \epsilon}, 1 - \tau -\alpha - \beta, \beta, 0] \newline
&amp;&amp; Q _{\rm outer2}(\epsilon, \delta, \alpha, \beta, \tau) = [0, \alpha, 1 - \tau -\alpha -\beta, \frac{\beta (\beta + \tau - \delta)}{\beta - \epsilon}, \frac{\beta(\delta - \epsilon) - \epsilon \tau}{\beta - \epsilon}] \newline
&amp;&amp; U _1(\epsilon, \delta, \tau, m) = \mathop{\max} _{\alpha + \beta \leq 1 - \tau, \frac{\epsilon \tau}{\delta -\epsilon} \leq \alpha, \beta} d _p (P _{\rm outer1} (\epsilon, \delta, \alpha, \beta, \tau) ^m , Q _{\rm outer1} (\epsilon, \delta, \alpha, \beta, \tau) ^m) \newline
&amp;&amp; L _2(\tau, m) = \mathop{\min} _{\frac{\epsilon \tau}{\delta - \epsilon} \leq  \alpha \leq 1 - \frac{\delta \tau}{\delta - \epsilon}} d _p(P _{\rm inner} (\alpha) ^m, Q _{\rm inner} (\alpha,  \tau) ^m)
\end{eqnarray}

(3 If $\delta + \epsilon &gt; 1$ and $\delta  - \epsilon \leq \tau \leq (\delta - \epsilon)/(2 -\delta - \epsilon)$, then we have:

\begin{eqnarray}
&amp;&amp; U _2(\epsilon , \delta, \tau, m) \leq d _p(P ^m, Q ^m) \leq L _3(\tau, m)
\end{eqnarray}

with

\begin{eqnarray}
&amp;&amp; P _{\rm outer2} (\epsilon, \delta, \alpha, \beta , \tau) = [\frac{\alpha (\delta - \epsilon) - (1 - \delta) \tau}{\alpha - (1- \delta)}, \frac{\alpha(\alpha + \tau - (1 - \epsilon))}{\alpha - (1 - \delta)}, 1 - \tau -\alpha - \beta, \beta, 0] \newline
&amp;&amp; Q _{\rm outer2}(\epsilon, \delta, \alpha, \beta, \tau) = [0, \alpha, 1 - \tau -\alpha -\beta, \frac{\beta (\beta + \tau - (1 - \epsilon))}{\beta - (1 - \delta)}, \frac{\beta(\delta - \epsilon) - (1 - \delta) \tau}{\beta - (1 - \delta)}] \newline
&amp;&amp; U _2(\epsilon, \delta, \tau, m) = \mathop{\max} _{\alpha + \beta \leq 1 - \tau, \frac{(1 - \delta) \tau}{\delta -\epsilon} \leq \alpha, \beta} d _p (P _{\rm outer2} (\epsilon, \delta, \alpha, \beta, \tau) ^m , Q _{\rm outer2} (\epsilon, \delta, \alpha, \beta, \tau) ^m) \newline
&amp;&amp; L _3(\tau, m) = \mathop{\min} _{\frac{(1-\delta) \tau}{\delta - \epsilon} \leq  \alpha \leq 1 - \frac{(1 - \epsilon) \tau}{\delta - \epsilon}} d _p(P _{\rm inner} (\alpha) ^m, Q _{\rm inner} (\alpha , \tau) ^m)
\end{eqnarray}


</div>

<h3 id="examples-binary-support">Examples: Binary support</h3>
<p>Consider distribution on a binary support. Given total variance, there’re two possible distribution pair. For instance,  $P = [0.4, 0.6], Q _1 = [0.3, 0.7], Q _2 = [0.5, 0.5]$, we have that $d _{p} (P,Q _1) = d _p (P, Q _2) = 0.1$, while $(P,Q _1)$ exhibits $(0.3 , 0.4) - $ mode collapse but $(P, Q _2)$ has no such mode collapse. Note that $(P, Q _2)$ has $(0.5, 0.6) - $ mode collapse and $(P, Q _1)$ has no such mode collapse. The simulation result is given below :</p>

<p><img src="http://slicerkao.github.io/images/rrrr.png" alt="alt text" /></p>

<p>As for the quantum case, things start to become more ineteresting. First, a two dimensional quantum state has three degree of freedom, which makes all the $\rho _x$ with the same $d _1(\rho, \rho _x)$ to be more than that of the classical case. Second, there are more possibilities (e.g. all the density matrices) for the mode collapse ‘region’ than the classical case ($\mu _0 = \vert 0 \rangle \langle 0 \vert \; , \; \mu _1 = \vert 1 \rangle \langle 1 \vert$).</p>

<p>For example, 
\begin{eqnarray}
&amp;&amp; \rho _{P} = 
\begin{pmatrix}
\frac{1}{2} &amp; 0 \newline
0 &amp; \frac{1}{2}
\end{pmatrix} \newline
&amp;&amp; \rho _{Q _1} = 
\begin{pmatrix}
\frac{1}{2} &amp; \frac{1}{2} \newline
\frac{1}{2} &amp; \frac{1}{2}
\end{pmatrix} \newline
&amp;&amp; \rho _{Q _2} = 
\begin{pmatrix}
1 &amp; 0 \newline
0 &amp; 0
\end{pmatrix}
\end{eqnarray}</p>

<p>The trace distance $0.5 \cdot \Vert \rho _{P} - \rho _{Q _1} \Vert _1 = 0.5 \cdot \Vert \rho _{P} - \rho _{Q _2} \Vert _1 = 0.5 $. Now classically, given  $\mu (0) = \vert 0 \rangle \langle 0 \vert \; , \; \mu (1) = \vert 1 \rangle \langle 1 \vert$ , $(\rho _{P}, \rho _{Q _2})$ exhibits $(0, 1/2)$-mode collapse while $(\rho _{P}, \rho _{Q _1})$ doesn’t. However, trace distance $d _1 (  \rho _{P}^m, \rho _{Q _1} ^m )$ and $d _1( \rho _{P}^m, \rho _{Q _2} ^m )$does not split as $m$ increases. In fact, careful calculation leads to :
\begin{eqnarray}
&amp;&amp; d _1(\rho _P ^m, \rho _{Q _1} ^m) = d _2(\rho _P ^m, \rho _{Q _2} ^m ) = (1 - \frac{1}{2 ^m})
\end{eqnarray}</p>

<p><img src="http://slicerkao.github.io/images/re-1.png" alt="alt text" /></p>

<p>But if we consider the quantum definition for mode collapse, we can see that both $(\rho _P, \rho _{Q _1})$ and $(\rho _P, \rho _{Q _2})$ exhibits $(0 , 1/2)$ mode collapse, and the latter mode collapse is achieved when $\mu = \begin{pmatrix} \frac{1}{2} &amp; \frac{-1}{2} \newline \frac{-1}{2} &amp; \frac{1}{2}\end{pmatrix}$. In fact, under the Bloch sphere representation, $(\rho _P, \rho _{Q _1})$ and $(\rho _P, \rho _{Q _2})$ are identical up to a unitary transform. To be more explicit, any two-dimensional quantum state $\rho$ (pure or mixed) can be represented in the Bloch representation:</p>

<p>\begin{eqnarray}
&amp;&amp; \rho = \frac{\mathbb{1} + \vec{r} \cdot \vec{\sigma}}{2} \newline
&amp;&amp; \vec{r} \in \mathbb{R}^3 \; , \; \Vert \vec{r} \Vert _2 \leq 1 \newline
&amp;&amp; \vec{\sigma} = (\sigma _x, \sigma _y, \sigma _z)
\end{eqnarray}</p>

<p>where $(\sigma _x, \sigma _y, \sigma _z)$ are the Pauli matrices. When $\vec{r} = \vec{0}$, it’s called a completely mixed state; when $\Vert \vec{r} \Vert _2 = 1$, it’s pure state. Notably, the trace distance between two arbitrary two-dimensional quantum state is:</p>

<p>\begin{eqnarray}
&amp;&amp; d _1(\rho _1, \rho _2) = \frac{\Vert \vec{r} _1 - \vec{r} _2\Vert _2}{2}
\end{eqnarray}</p>

<p>Also, trace distance is unitarily invariant : $d _1(\rho _1, \rho _2) = d _1(U  \rho _1 U ^{\ast}, U  \rho _2 U ^{\ast})$. So the above example is a special case for the following theorem:</p>

<div class="theorem">
$\; \rho _0 = \frac{\mathbb{1}}{2}$ and $\rho _r$ is a density matrix with $\vec{r}$. Then, 
\begin{eqnarray}
d _1 (\rho _0 ^m, \rho _r ^m) = \frac{1}{2} \cdot \sum _{k = 0} ^{m} {m \choose k} \vert (\frac{\Vert \vec{r} \Vert _2 + 1}{2})^{m-k}( \frac{1 - \Vert \vec{r} \Vert _2}{2})^k - \frac{1}{2 ^m}\vert
\end{eqnarray}
</div>

<p>One can check that when $\Vert \vec{r} \Vert _2 = 1$, $d _1(\rho _0 ^m , \rho _r ^m) = 1 - \frac{1}{2 ^m}$, which coincides with the result.</p>

<p>To show a signficant distance split in 2 dimensional cases, we consider the example in the following figure. It shows the possible region of $d _1(\rho ^m, \rho _x ^m)$ among all possible $\rho _x$ such that $d _1(\rho, \rho _x ) = 0.25$, and $\rho$ is randomly selected on a $0.5-$ radius Bloch sphere.</p>

<p><img src="http://slicerkao.github.io/images/re.png" alt="alt text" /></p>

<p>To coincide the split with Zinan’s work, we adopts the geometric approach as in Zinan’s original paper. We’d like to see what is the quantum counterpart and characterisitc of the so called mode collapse region.</p>

<h3 id="mode-collapse-region">Mode collapse region</h3>

<p>Mode collapse region is defined as :</p>

<div class="definition">
Let $\rm conv$ denotes the convex hull of a set. Then the mode collapse region
is defined as:
\begin{eqnarray}
&amp;&amp; \mathcal{R} (P,Q) = {\rm conv} \big( (\epsilon, \delta) | \delta  &gt; \epsilon
\; {\rm and \; P \;, \;Q \; has } \; (\epsilon , \delta)- {\rm mode \; collapse}\big)
\end{eqnarray}
</div>

<p>Likewise, a quantum counterpart for mode collapse region can be defined as:</p>

<div class="definition">
Let $\rm conv$ denotes the convex hull of a set. Then the quantum mode collapse region
is defined as:
\begin{eqnarray}
&amp;&amp; \mathcal{R} _q (\rho _P, \rho _Q) = {\rm conv} \big( (\epsilon, \delta) | \delta  &gt; \epsilon
 \; {\rm and \; \rho _P \;, \;\rho _Q \; has } \; (\epsilon , \delta)- {\rm mode \; collapse}\big)
\end{eqnarray}
</div>

<p>Let’s investigate both the classical and quantum mode collaspe region for binary support distribution. Consider the example $(P,Q_1)$ in the previous section, the classical mode collapse region is plotted as below:</p>

<p><img src="http://slicerkao.github.io/images/classical.png" alt="alt text" /></p>

<p>And the quantum mode collapse region is plotted as below:</p>

<p><img src="http://slicerkao.github.io/images/quantum.png" alt="alt text" /></p>

<p>As from the experiment result, both the classical and quantum mode collapse
region are the same. In fact, we can prove the following theorem:</p>

<div class="theorem">
For arbitrary classical distribution pair $(P, Q)$ and their corresponding (diagonal) quantum state $(\rho _P, \rho _Q)$, we have:

\begin{eqnarray}
&amp;&amp; \mathcal{R}(P,Q) = \mathcal{R} _q (\rho _P, \rho _Q)
\end{eqnarray}
</div>

<p>How do we associate the mode collapse region with the divergence bound ? In
Zinan’s work, he first observe that if $\mathcal{R} (P _1,Q _1) \subset
\mathcal{R} (P _2, Q _2) $, then $d _p(P _1,Q _1) \leq d _p(P _2, Q _2)$. Thus, the problem of bounding divergence is associated with bounding the mode collpase region.</p>

<p>Furthermore, a key lemma for bounding the mode collapse region is resulted from an implication of the Blackwell’s theorem :</p>

<p>\begin{eqnarray}
&amp;&amp; \mathcal{R} (P _1, Q _1) \subset \mathcal{R} (P _2, Q _2) \rightarrow
\mathcal{R} (P _1 ^m, Q _1 ^m) \subset \mathcal{R} (P _2 ^m, Q _2 ^m) \; \forall \; m
\end{eqnarray}</p>

<p>Interestingly, a quantum counterpart of the Blackwell’s theorem: _ <a href="https://arxiv.org/abs/quant-ph/0410233">the Shmaya’s theorem</a> leads to a same implication for quantum mode collapse region. We can prove that:</p>

<div class="theorem">
\begin{eqnarray}
&amp;&amp; \mathcal{R} _q (\rho _{P _1} , \rho _{Q _1}) \subset \mathcal{R} _q (\rho _{P _2}, \rho _{Q _2}) \rightarrow
\mathcal{R} _q( \rho _{P _1} ^m, \rho _{Q _1} ^m) \subset \mathcal{R} _q (\rho _{P _2} ^m, \rho _{Q _2} ^m) \; \forall \; m
\end{eqnarray}

</div>

<p>Combining the above two theorem, the bound obtained by Zinan in the classical literature is actually applicable for the quantum case. For instance, consider the last experiment in the previous section, we can almost tightly bound all the possible value region for $d _1 (\rho ^m, \rho _x ^m )$ with $\rho _x$ satisfying $d _1(\rho, \rho _x) = 0.25$:</p>

<p><img src="http://slicerkao.github.io/images/preder.png" alt="alt text" /></p>

<p>As the result, all the split phenomenon caused by mode collapse in the classical setting can be adopted in the quantum counterpart.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Study Note: Understanding Generative Adversarial Net (1)]]></title>
    <link href="http://slicerkao.github.io/blog/2018/07/11/study3/"/>
    <updated>2018-07-11T21:25:48+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/07/11/study3</id>
    <content type="html"><![CDATA[<p>Recently a bunch of Generative Adversarial Net (GAN) related research has boomed in the unsupervised learning community, and it seems interesting to fully understand the mechanism of this novel learning framework. Just a few days ago (July 5th, 2018), Alexia Jolicoeur-Martineau et al proposed a <a href="https://arxiv.org/abs/1807.00734">Relativistic GAN</a> that aimed to provide improvement via changes of the discriminator term in the original GAN objective function. It pointed out that the real data are usually ignored during the learning process in most GAN paradigm, and thus it proposed a relativistic quantity to measure how ‘real’ the real samples are compared to the fake sample.</p>

<p><img src="http://slicerkao.github.io/images/de.png" alt="alt text" /></p>

<!--more-->

<p>It seems to to me that the original goal of GAN is to train the followings given the sampled real data sets $x_i \mathop{\sim}^{i.i.d} P$:</p>

<ul>
  <li>
    <p>A generator $G_{\theta}(z)$ that has small distributional divergence with respect to the true distribution. This is the main goal of the generative learning paradigm.</p>
  </li>
  <li>
    <p>A discriminator $D_{\omega}$ equipped with strong detection ability. This is rather a side product for GAN, but the training procedure relies on the detection result (e.g. the backpropagation for weight update.)</p>
  </li>
</ul>

<h3 id="generator-perspective-minimization-of-the-divergence">Generator perspective: minimization of the divergence</h3>

<p>For the first part, most GAN objective functions are some $f$ divergence with the following forms:</p>

<p>\begin{eqnarray}
&amp;&amp; D_f(P||Q) = \int_{\chi} q(x)f \Big( \frac{p(x)}{q(x)}\Big) dx
\end{eqnarray}</p>

<p>As pointed out in the <a href="https://arxiv.org/abs/1606.00709">$f$ GAN literature</a>, the $f$ divergence is approximated via a variational lower bound known as Frenchel conjugate $f^\ast$ (for all lower-semicontious function $f$). This conjugate is convex with the defintion:</p>

<p>\begin{eqnarray}
&amp;&amp; f^\ast (t) = \mathop{\sup}_{u \in {\rm dom}_f} [ut - f(u)]
\end{eqnarray}</p>

<p>Likewise, $f$ can be expressed in terms of $f^\ast$ since the pair $(f, f^\ast)$ are dual to each other. Plugging into the expression of the $f$ divergence, we have:</p>

<p>\begin{eqnarray}
&amp;&amp; D_f(P||Q) = \int_{\chi} q(x) \mathop{\sup}_{t \in {\rm
dom} _{f^\ast}} \Big( t\frac{p(x)}{q(x)} - f^\ast(t) \Big) dx \newline 
&amp;&amp; \geq \sup _{T(x) \in \mathcal{T}} \Big( \int _{\chi} p(x) T(x) dx - \int _{\chi} q(x) f^\ast (T(x)) \Big) \newline
&amp;&amp; = \sup _{T(x) \in \mathcal{T}} (E _{x \sim P}[T(x)] - E _{x \sim Q}[f^ \ast (T(x))])
\end{eqnarray}</p>

<p>To achieve the tightest lower bound, $T(x) = \frac{df}{dx} \big(\frac{p(x)}{q(x)}\big)$. For instance, consider a $f$ divergence with $f(u) = u \log u - (u+1) \log (u+1)$, first compute the Frenchel dual as :</p>

<p>\begin{eqnarray}
&amp;&amp; f^\ast (t) = \mathop{\sup} _{u &gt; 0} [ut - u \log u + (u+1) \log(u+1)] \newline
&amp;&amp; = - (t + \log (10^{-t} - 1)) \; , \; t &lt; 0 
\end{eqnarray}</p>

<p>So the variational lower bound for the $f$ divergence becomes :</p>

<p>\begin{eqnarray}
&amp;&amp; \sup _{T(x) \in \mathcal{T}} (E _{x \sim P}[T(x)] + E _{x \sim Q}[\log (1 - exp(T(x)))])
\end{eqnarray}</p>

<p>Standard GAN adopts $T(x)$ with the form :</p>

<p>\begin{eqnarray}
&amp;&amp; T(x) = \log D_\omega(x) = 1/(1 + e^{-V_{\omega}(x)})
\end{eqnarray}</p>

<p>So the loss is the familiar form :</p>

<p>\begin{eqnarray}
&amp;&amp; L(\theta, \omega) = \mathop{\sup} _{D} [E _{x \sim P}[\log D _{\omega}(x)] + E _{x \sim G _{\theta}(P_z)}[\log (1 - D _{\omega}(x))]]
\end{eqnarray}</p>

<p>When $D(x)$ is optimal, it has $D(x) = \frac{p(x)}{p(x) + G_{\theta} (P_z)(x)}$, or with $Q_{\theta} = G_{\theta}(P_z)$ as a shorthand, $D(x) = \frac{p(x)}{p(x) + q_{\theta}(x)}$. Plugging it back to the variational bound, one can recover the J-S divergence in the literature. Thus with a perfect discriminator, the generator can be trained to minimize the divergence loss between generated distribution and the target distribution.</p>

<h3 id="discriminator-perspective-optimal-detector">Discriminator perspective: Optimal Detector</h3>

<p>The above arguments stand on a generator perspective, where the goal is to
minmize a divergence measure (with high fidelity). On the other hand, we’ve seen that the optimal discriminator to construct JS divergence is actually a maximum likelihood detector. ML is also an optimal detector for hypthothesis testing with uniform prior, where the performance metric is the Bayesian risk. Such a correspondence between divergence measure and the optimal Bayesian detector is not a coincidence: The form of the standard GAN variational lower bound takes a log-likelihood expression. i.e, when jointly maximize the log of true alarm and negative probability, one get a J-S divergence measure with a Bayesian optimal detector.</p>

<p>For more example, consider a variational objective with $0-1$ loss, i.e.,</p>

<p>\begin{eqnarray}
&amp;&amp; L(\theta, \omega) = \mathop{\sup} _{D : \chi \to 0, 1} [ E _{x \sim P}[\mathbb{1}[D _{\omega}(x)]] + 
E _{G _{\theta}(z) \sim Q}[1 - \mathbb{1}[D _{\omega}(G _{\theta}(x))]]]
\end{eqnarray}</p>

<p>This loss is exactly (up to a 1/2 prior probability) related to the Bayesian risk, and the maximization of it leads to a divergence measure called total variance $d_{\rm TV}(P,Q)$, defined as:</p>

<p>\begin{eqnarray}
&amp;&amp; d_{\rm TV}(P,Q) = \mathop{\sup}_{S \subset \chi} [P(S) - Q(S)] = L(\theta, \omega^\ast) - 1
\end{eqnarray}</p>

<p>So to construct an optimal detector in a sense of the Baysian risk, the discriminator should jointly maximize the true positive and negative probability. And its maximum is $1 + d _{\rm TV}(P,Q)$. Likewise, standard GAN jointly maximize the log likelihood of true positive and negative probabilty, which yields an optimal value expressed by the J-S divergence. For other GAN, one can also trace the decision region of the optimal discriminator, and may understand if such a GAN bias toward true positive or true negative probability.</p>

<h3 id="dose-gan-bias-toward-true-positive-or-true-negative-during-the-training-process-">Dose GAN bias toward true positive or true negative during the training process ?</h3>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Briefing : Derivation of the uniform convergence]]></title>
    <link href="http://slicerkao.github.io/blog/2018/06/21/study2/"/>
    <updated>2018-06-21T15:57:36+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/06/21/study2</id>
    <content type="html"><![CDATA[<p>The following notes are summarized from Prof I-Hsiang Wang’s lecture: <a href="http://homepage.ntu.edu.tw/~ihwang/Teaching/Sp18/MPML.html"> Mathematic  Principles  of  Machine  learning</a>, Lec 3. An alternated derivation is also given in the <a href="http://nowak.ece.wisc.edu/SLT09/lecture19.pdf">website note</a> and summarized here.</p>

<!--more-->

<h3 id="erm-and-the-uniform-deviation">ERM and the Uniform deviation</h3>

<p>Given empirical dataset $\mathcal{D}$, ERM leads to a solution $f_{\rm ERM} \in {\rm argmin}_{f \in \mathcal{F}} \hat{L} _{\mathcal{D} _n}(f; P)$. It can be shown that the estimation error $\Xi _{\mathcal{F}} (f _{\rm ERM}) = L(f _{ \rm ERM}; P) - \mathop{\inf} _{f \in \mathcal{F}}L(f;P)$ is upper bounded by the uniform deviation:</p>

<p>\begin{eqnarray}
&amp;&amp; \Xi _{\mathcal{F}}(f _{\rm ERM} ; P) \leq 2 \mathop{\sup} _{f \in \mathcal{F}} |\hat{L} _{\mathcal{D} _n} (f; P) - L(f ; P)|
\end{eqnarray}</p>

<p>Let the r.h.s of the inequality (withdraw the constant 2 and the supremum sign) be denoted by $\Gamma _n (f) = \hat{L} _{\mathcal{D} _n} (f; P) - L(f ; P)$. The goal is then to bound the probability $P[\sup _{f \in \mathcal{F}} \vert \Gamma _n (f) \vert  \geq t]$, which is first upper bounded by the upper and the lower deviation:</p>

<p>\begin{eqnarray}
&amp;&amp; P[\sup _{f \in \mathcal{F}} \vert \Gamma _n (f) \vert  \geq t] \leq P[\sup _{f \in \mathcal{F}} \Gamma _n (f)  \geq t] + P[\sup _{f \in \mathcal{F}} [- \Gamma _n (f)] \geq t] 
\end{eqnarray}</p>

<p>Now that $\sup _{f \in \mathcal{F}}\Gamma _n (f) = \gamma(Z _1, \dots , Z _n) $ is a function of random
variable for training sample $(Z _1, \dots Z _n)$, and the variation of the
function by random variable is bounded above by:</p>

<p>\begin{eqnarray}
&amp;&amp; |\gamma(z _1, z _2, \dots , z _i, \dots , z _n) - \gamma(z _1, z _2, \dots , \tilde{z} _i, \dots , z _n)| \leq \frac{C}{n} 
\end{eqnarray}</p>

<p>for loss value constrained within a interval with length $C$. McDiarmid’s
inequality $P[ f(Z) - E[f(Z)] \geq t ] \leq {\rm exp} (\frac{-2 t^2}{\sum c^2 _k})$ then applies for $c _k = \frac{C}{n}$. Since we consider loss to be constrained within length-1 interval, this yields the following bound :</p>

<p>\begin{eqnarray}
&amp;&amp;  P \, [\sup _{f \in \mathcal{F}} \Gamma _n (f) \geq E[\sup _{f \in \mathcal{F}} \Gamma _n] + t] \leq {\rm exp} (-2nt^2)<br />
\end{eqnarray}</p>

<p>Now we already have a upper bound for the probability of large supremum deviation. What’s left is to upper bound the quantity $E[\sup _{f \in \mathcal{F}} \Gamma _n]$ so that we got:</p>

<p>\begin{eqnarray}
&amp;&amp; P \, [\sup _{f \in \mathcal{F}} \Gamma _n (f) \geq {\rm some \; upper \; bound} + t ] \leq  P \, [\sup _{f \in \mathcal{F}} \Gamma _n (f) \geq E[\sup _{f \in \mathcal{F}} \Gamma _n] + t] \leq {\rm exp} (-2nt^2)<br />
\end{eqnarray}</p>

<h3 id="rademacher-complexity">Rademacher Complexity</h3>

<p>To upper bound the term $E _{D ^n}[\sup _{f \in \mathcal{F}} \Gamma _n]$, note that :
\begin{eqnarray}
&amp;&amp; \mathop{\sup} _{f \in \mathcal{F}} \Gamma _n(f) = \sup _{g \in \ell \circ \mathcal{F}} [\frac{1}{n} \sum _i ^n g(Z _i) - E[g(Z)]]
\end{eqnarray}</p>

<p>Where $\ell$ is the loss function, and $Z$ is adopted in the previous section as $Z = (X,Y)$. Vapnik et.al introduce some ghost sample $ \tilde{D} = (\tilde{Z} _1 \dots \tilde{Z} _n) \sim P^{\otimes n}$ so that $E _{\tilde{D} _n} [\frac{1}{n} \sum _{1}^{n} g(\tilde{Z} _i)] = E(g(Z))$. Introducing this we get:</p>

<p>\begin{eqnarray}
&amp;&amp; E _{D^n}[\mathop{\sup} _{f \in \mathcal{F}} \Gamma _n(f)] = E _{D^n} [\sup _{g \in \ell \circ \mathcal{F}} [\frac{1}{n} \sum _i ^n g(Z _i) - E[g(Z)]]
] \newline
&amp;&amp; = E _{D^n} [\sup _{g \in \ell \circ \mathcal{F}} E _{\tilde{D}^n}
[\frac{1}{n} \sum _i ^n g(Z _i) - g(\tilde{Z} _i)]] \newline
&amp;&amp; \leq E _{D^n} \; E _{\tilde{D}^n} [\sup _{g \in \ell \circ \mathcal{F}}
\frac{1}{n} \sum _i ^n g(Z _i) - g(\tilde{Z} _i)] \newline
\end{eqnarray}
To this end, one can further upper bound it by $E _{D _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} g(Z _i)] + E _{\tilde{D} _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} -g(\tilde{Z} _i)] $, but this leads the analysis to nowhere since the supremum of a same term with different sign has different result with respect to the expectation. To resolve it, introduce a symmetrization random variable (like adding extra term for the summation (integral) expansion expression of the original expectation value) to equalize the value of the two terms. More explicitly, the random variable $g(Z _i) - g(\tilde{Z} _i)$ has a symmetric distribution, thus introducing i.i.d random variables $\sigma _i \sim {\rm Ber} (\frac{1}{2})$ to the expectation won’t change its value. Thus,</p>

<p>\begin{eqnarray}
&amp;&amp; E _{D^n} \; E _{\tilde{D}^n} [\sup _{g \in \ell \circ \mathcal{F}}
\frac{1}{n} \sum _i ^n g(Z _i) - g(\tilde{Z} _i)] \newline
&amp;&amp; = E _{D^n} \; E _{\tilde{D}^n} \; E _{\sigma _1 , \sigma _2 , \dots , \sigma _n } [\sup _{g \in \ell \circ \mathcal{F}}
\frac{1}{n} \sigma _i [\sum _i ^n g(Z _i) - g(\tilde{Z} _i)]] \newline
&amp;&amp; \leq E _{D _n} E _{\sigma _1 \dots \sigma _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(Z _i)] +  E _{\tilde{D} _n} E _{\sigma _1 \dots \sigma _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} - \sigma _i g(\tilde{Z} _i)] \newline
&amp;&amp; = 2 E _{D _n} E _{\sigma _1 \dots \sigma _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(Z _i)] 
\end{eqnarray}</p>

<p>The quantity on the right hand size is called the Rademacher complexity
$\mathcal{R} _n (\mathcal{G}) $ , which is $E _{D _n} E _{\sigma _1 \dots \sigma _n} [\mathop{\sup} _{g \in \mathcal{G}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(Z _i)] $</p>

<p>Before proceeding toward the property of the Rademacher complexity, we can
relate the complexity measure back to the estimation error via :</p>

<p>\begin{eqnarray}
&amp;&amp; P[\Xi _{\mathcal{F}}(f _{\rm ERM} ; P) \geq 4 \mathcal{R} _n(\mathcal{G}) + \epsilon] \leq P[\mathop{\sup} _{f \in \mathcal{F}}|\Gamma _n(f)| &gt; 2 \mathcal{R} _n(\mathcal{G}) + \frac{\epsilon}{2}] \leq 2 {\rm exp} \big( - \frac{n \epsilon^2}{2} \big) 
\end{eqnarray}</p>

<p>Also, with Mcdiamard inequality, one can upper bound the Rademahcer complexity by the empirical Rademacher inequality $\hat{\mathcal{R}} _n (d _n , \mathcal{G}) = E _{\bf \vec{\sigma}} [ \mathop{\sup} _{g \in \mathcal{G}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(z _i)]$ with probability larger than $1-\delta$ :</p>

<p>\begin{eqnarray}
&amp;&amp; \mathcal{R} _n( \mathcal{G}) \leq 2 \hat{\mathcal{R}} _n (d _n , \mathcal{G}) + 3 \sqrt{\frac{2}{n} \log \big(\frac{3}{\delta}\big)}<br />
\end{eqnarray}</p>

<h3 id="binary-classification-0-1-loss-and-the-vc-dimension">Binary Classification, 0-1 loss and the VC dimension</h3>

<p>The empirical Rademacher complexity can be further related to the functional
class $\mathcal{F}$ in the binary classification setting, with $0-1$ loss:</p>

<p>\begin{eqnarray}
&amp;&amp; \hat{\mathcal{R}} _n (d _n; \mathcal{G}) = E _{\bf \vec{\sigma}} [ \mathop{\sup} _{g \in \mathcal{G}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(z _i)] \newline
&amp;&amp; = E _{\bf \vec{\sigma}} [ \mathop{\sup} _{f \in \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i \ell (f(x _i, y _i))] \newline
&amp;&amp; = E _{\bf \vec{\sigma}} [ \mathop{\sup} _{f \in \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i \frac{1 - y _i f(x _i)}{2}] \newline
&amp;&amp; = \frac{1}{2}E _{\bf \vec{\sigma}} [ \mathop{\sup} _{f \in \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i f(x _i)] = \frac{1}{2} \hat{R} _n(x _1, \dots x _n; \mathcal{F})
\end{eqnarray}</p>

<p>The above expression has a good convenience: We can simplify the supremum term into maximum of a finite functional class, since the original functional class can be partitioned into groups $\mathcal{A}$ that evaluate the same on the empirical data set $(x _1 \dots x _n)$. Hence,</p>

<p>\begin{eqnarray}
&amp;&amp; \hat{R} _n(x _1, \dots x _n; \mathcal{F}) = E _{\bf \sigma} \big[ \mathop{max} _{u \in \mathcal{A}} \frac{1}{n} \sum _{i = 1}^{n} \sigma _i u _i \big]
\end{eqnarray}</p>

<p>The size of the set $\mathcal{A}$ is upper bounded by the growth function $\tau_{\mathcal{F}}(n)$, which is defined as:</p>

<p>\begin{eqnarray}
&amp;&amp; \tau _{\mathcal{F}} (n) = \mathop{max} _{(x _1 \dots x _n)} \tau _{\mathcal{F}} (x _1 \dots x _n)= \mathop{max} _{(x _1 \dots x _n)} \big\vert [(f(x _1) \dots f(x _n)) | f \in \mathcal{F}] \big\vert
\end{eqnarray}</p>

<p>With this one can adopt the Massart’s finite Lemma : If $\mathcal{A}$ is a
finite set, then :</p>

<p>\begin{eqnarray}
&amp;&amp; E _{\bf \sigma} \big[ \mathop{max} _{u \in \mathcal{A}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i u _i  \big] \leq \frac{\rm max _{u \in \mathcal{A}} \Vert u \Vert}{n} \sqrt{2 \log |\mathcal{A}|}
\end{eqnarray}</p>

<p>(which is proven by maximum of subgaussian). Direct application of this lemma leads to :
\begin{eqnarray}
&amp;&amp; \hat{\mathcal{R}} _n (\bf x = (x _1 \dots x _n) ; \mathcal{F}) \leq \sqrt{\frac{2 \log \tau _{\mathcal{F}}(\bf x)}{n}}, \; \; \; \mathcal{R} _n(\mathcal{F}) \leq \sqrt{\frac{2 \log \tau _{\mathcal{F}} (n)}{n}}
\end{eqnarray}</p>

<p>Now we can introduce the VC dimension, which is defined as:
\begin{eqnarray}
&amp;&amp; VC(\mathcal{F}) = {\rm max}[n : \tau _{\mathcal{F}} (n) = 2^n]
\end{eqnarray}</p>

<p>Rethink about the meaning of the growth function $\tau _{\mathcal{F}} (n)$ in the case $n$ greater or less than the VC dimension $d$ of the functional class, we could arrive at the Sauer’s lemma:</p>

<p>\begin{eqnarray}
&amp;&amp; \tau _{F} (n) \leq \sum _{i =0}^{ \mathop{min} (n, d)} {n \choose i} \longrightarrow ({\rm which \; is \; further \; upper \; bounded \; by \;}  (n + 1) ^d)
\end{eqnarray}</p>

<p>Hence, for 0-1 loss binary classification task, we have with probability at
least $1- \delta$:
\begin{eqnarray}
&amp;&amp; \Xi _{\mathcal{F}}(f _{ERM} ; P) \leq 2 \sqrt{\frac{2 VC(\mathcal{F}) \log(n+1)}{n}} + \sqrt{\frac{2}{n} \log (\frac{2}{\delta})}
\end{eqnarray}</p>

<h3 id="relaxation-to-non-0-1-loss-and-non-binary-case">Relaxation to non 0-1 loss and non binary case</h3>
<p>From the above analysis, it’s assumed that the label cardinality $\vert
\mathcal{Y} \vert$ is finite so that $\vert \mathcal{A} \vert \leq \vert \mathcal{Y} \vert ^n$ is finite and the Massart finite lemma apply. Also, it’s assumed that the loss function is hard $0-1$, and thus the functional class can be perfectly grouped. It may not be the case when the loss function is real valued.</p>

<p>Having Massart’s lemma in mind, we still want to discretize the functional
class. What can be done is to introduce the covering number. Define a pseudo
distance $\rho _{z _1 \dots z _n} (f,g) = \sqrt{\frac{1}{n} \sum _{i=1}^n |f(z _i) - g(z _i)|^2}$, it satisfies triangle inequality and symmetry property. But it’s not a metric since indistinguishability is not guaranteed : $\rho( f, g) = 0$ does not imply $ f = g$. An $\epsilon -cover$ of a functional class $\mathcal{G} \subset \mathcal{U}$ (($\mathcal{U}$, $\rho$) is a pseudometric space) is defined as a subset $\mathcal{C} \subset \mathcal{U}$ such that for all $g \in \mathcal{G}$, there exist $h \in \mathcal{C}$ that $\rho (g, h) \leq \epsilon$. The covering number is then defined as:</p>

<p>\begin{eqnarray}
&amp;&amp; N(\epsilon, \mathcal{G}, \rho) = {\rm min}[|mathcal{C}|: \mathcal{C} \; is \; an \; \epsilon -cover \; of \; \mathcal{G}]
\end{eqnarray}</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Briefing : PAC learnibility]]></title>
    <link href="http://slicerkao.github.io/blog/2018/06/19/study-note-2/"/>
    <updated>2018-06-19T16:47:04+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/06/19/study-note-2</id>
    <content type="html"><![CDATA[<p>The following notes are summarized from Prof I-Hsiang Wang’s lecture: <a href="http://homepage.ntu.edu.tw/~ihwang/Teaching/Sp18/MPML.html"> Mathematic  Principles  of  Machine  learning</a>, covering lecture 1 and 2.</p>

<!--more-->

<h3 id="discriminative-learning-consistency-and-no-free-lunch">Discriminative learning, Consistency and No free lunch</h3>

<p>In the setting of discriminative learning, <em>excessive risk</em> $R_{exessive}$ is decomposed into estimation error (which is associated with sampling bias) and the approximation error (which is associated with the size of the functional class $\mathcal{F}$) as :</p>

<p>$R_{excessive}  = L(f, P)- L(f^* ,P) = (L(f, P) - \mathop{\inf}_{f_n \in \mathcal{F}} L(f_n, P)) \longrightarrow \; {\rm estimation \; error}$</p>

<p>$+ \; ( \mathop{\inf}_{f_n \in \mathcal{f}} L(f_n, p) - L(f^* , p))
\longrightarrow \; {\rm approximation \; error}$</p>

<p>where $f^{\ast}$  denotes the Baysian optimal rule. The term <em>consistency</em> for a sequence of learning algorithms $f_n $ and a distribution $\mathcal{P}$ is defined as:
\begin{eqnarray}
&amp;&amp; L(f_n ; P) - L(f^* ; P) \mathop{\rightarrow}^{L_1}0 \; as \; n \rightarrow \infty
\end{eqnarray}</p>

<p>And with loss bounded from above, by dominate convergence theorem the
consistency criteria is equivalent to convergence in probability:
\begin{eqnarray}
\forall \, \epsilon &gt; 0, \; \mathop{\lim}_{n \leftarrow \infty} P^{\otimes n}[|L(f_n; P)- L(f^*; P)| &lt; \epsilon ] = 1 
\end{eqnarray}</p>

<p>Impossibility result holds for general learning paradigms as well as
discriminative paradigm. No free lunch theorem states the existence of bad
distribution such that small excessive risk can’t be achieved with finite
samples. Therefore discriminative approach confine the functional group to circumvent bad convergent behaviour. This derives the concept of PAC learnibility: There’re some functional class being learnbable, and some that are not.</p>

<p>Note that the lower bound for the imposibility results is established via a key observation: If a
distribution $\mathcal{P}$ is concentrated on a finite set of data $X$ with $P(Y|X)$ being deterministic , then the algorithm output can be highly biased. It’s due to the fact that the training data may not cover all of the element in $X$, which make the algorithm only conduct random guess for the unseen data. (i.e. $f(X_1,Y_1, X_2, Y_2 \dots X_n, Y_n) \perp Y_{\rm test} $)</p>

<h3 id="pac-learnable-realizability">PAC learnable, realizability</h3>

<p>In the discriminative paradigm we aim to minimize the estimation error $\Xi_{\mathcal{F}}(L,P) = L(f,P) -
\mathop{\inf}_{f_n \in \mathcal{F}}L(f_n, P)$, and we’re interested in the
number of samples required to achieve certain ($\epsilon, \delta $)
generalization guarantee. Define the sample complexity of a hypothesis class
$\mathcal{F}$ as</p>

<p>\begin{eqnarray}
&amp;&amp; n_{\mathcal{F}}(\epsilon, \delta) \mathop{=}^{\Delta} {\rm min}[n: \, \exists f_n \in \mathcal{F} \, {\rm s.t.} \, \forall P, \, P^{\otimes n}[\Xi_{\mathcal{F}}(f_n, P) \leq \epsilon] \geq 1-\delta]
\end{eqnarray}</p>

<p>So if there’s some learning algorithm $f_n \in \mathcal{F}$ that guarantee estimation error to be less than
$\epsilon$ with high confident level and finite sample for all distribution,
than such a functional class in PAC learnable.</p>

<p>ERM rule provide a sufficient condition of PAC learnability, via 
\begin{eqnarray}
&amp;&amp; \Xi_{\mathcal{F}}(f_{ERM},P) \leq 2 \mathop{\sup}_{f\in \mathcal{F}}|\Delta
(f,P)|
\end{eqnarray}</p>

<p>Where $\Delta (f, P)$ is the deviation between true risk and the empirical risk.</p>

<p><em>Realizability</em> indicates a existance of $f \in \mathcal{F}$ such that L(f,P) is
zero, which is equivalent to $\ell (f(X), Y) = 0$ with probability one. In this
case:</p>

<ul>
  <li>
    <p>ERM always achieve zero emprirical risk.</p>
  </li>
  <li>
    <p>$f^{*}$ is a deterministic fuction</p>
  </li>
</ul>

<p>Thus any <em>bad</em> function $f_{\rm bad} \in \mathcal{F}$ is less probable to be
picked out from ERM rule. This indeed provided a sample commplexity with smaller
scaling factor $\mathcal{O}(\epsilon^{-1})$, as compared with the agnostic case
$\mathcal{O}(\epsilon^{-2})$</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Study Note: Max of subgaussian]]></title>
    <link href="http://slicerkao.github.io/blog/2018/06/19/study-note/"/>
    <updated>2018-06-19T14:46:06+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/06/19/study-note</id>
    <content type="html"><![CDATA[<p>Given $n$ independently distributed sub-gaussian random variable $Z_i$, each has bounded MGF due to sub gaussian property:</p>

<p>\begin{eqnarray}
&amp;&amp; E[e^{Z_i}] \leq e^{\lambda^2 \sigma^2 /2 } \;\;\; \forall i = 1 \dots n
\end{eqnarray}</p>

<p>for some $\lambda$ being postive real number. Let $X = \mathop{\max}_{1\leq i \leq n } Z_i$, one has that:</p>

<p>\begin{eqnarray}
&amp;&amp; E[X] \leq \sigma \sqrt{2 \log n} 
\end{eqnarray}
<!--more--></p>

<p>To prove this, note that the aim is to upper bound the expected value $E[X]$; thus Jensen’s inequality comes in handy:</p>

<p>\begin{eqnarray}
&amp;&amp; e^{t E[X]} \leq E[e^{t X}] = E[\mathop{\max}_{1\leq i \leq n} e^{t Z_i}] \leq \sum _{i=1} ^{n} E[e^{t Z_i}] \leq n e^{t^2 \sigma^2 /2}
\end{eqnarray}</p>

<p>Thus 
\begin{eqnarray}
&amp;&amp; E[X] \leq \frac{\log n}{t} + t \sigma ^2 /2 \leq \sigma \sqrt{2 \log n} 
\end{eqnarray}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Briefing: Quantum Adiabatic Search (2001)]]></title>
    <link href="http://slicerkao.github.io/blog/2018/04/29/study-note-quantum-adiabatic-search-2001/"/>
    <updated>2018-04-29T18:29:47+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/04/29/study-note-quantum-adiabatic-search-2001</id>
    <content type="html"><![CDATA[<p>In this post, a quantum adiabatic method applied to search problem is summarized and commented. All the following context is based on <a href="https://arxiv.org/abs/quant-ph/0107015">$\rm Quantum \, Search \, by \, Local \, Adiabatic \, Evolution$</a> from J$\acute{e}$r$\acute{e}$mie Roland and Nicolas J. Cerf.</p>

<!--more-->

<h3 id="adiabatic-theorem">Adiabatic Theorem</h3>

<p>Given a time dependent Hamiltonian $H(t)$, let $\vert E_k \, ; t \rangle$ be the eigenstate of $H(t)$ such that:</p>

<p>\begin{eqnarray}
&amp;&amp; H(t)|E_k \, ; t\rangle = E_k(t)|E_k \, ; t\rangle
\end{eqnarray}</p>

<p>where $E_k$ denotes the k-th eigenvalue of the Hamiltonian. Also, define the minimum eigengap $g_{min}$ as:</p>

<p>\begin{eqnarray}
&amp;&amp; g_{min} = \mathop{\min}_{0 \leq t \leq T}|E_1(t)-E_0(t)| 
\end{eqnarray}</p>

<p>Then one version of the adiabatic theorem states that if $g_{min}$ within the evolution time $T$ is not too small (compared to the variation of the Hamiltonian), then the initial state $\vert E_0 \, ; t\rangle$ evolves to $\psi(T)$ that’s close to $\vert E_0 \, ; T\rangle$. Intuitively, larger eigengap implies less quantum transition from base state to the first activated state, and thus the wave fuction “stay” at the base state throughout the evolution.</p>

<p>Formally, the variational quantity of $H$ is denoted by $\langle \frac{dH}{dt} \rangle_{1,0}$ as:</p>

<p>\begin{eqnarray}
&amp;&amp; \langle \frac{dH}{dt} \rangle_{1,0} = \langle E_1;t | \frac{dH}{dt}|E_0;t\rangle
\end{eqnarray}</p>

<p>Then provided that</p>

<p>\begin{eqnarray}
&amp;&amp; \frac{\langle \frac{dH}{dt}\rangle_{1,0}}{g^2_{min}} \leq \epsilon
\end{eqnarray}</p>

<p>one could evolve the state $\vert E_0;0\rangle$ toward state $\psi(T)$ such that:</p>

<p>\begin{eqnarray}                                                               <br />
&amp;&amp; |\langle E_0;T|\psi(T)|^2 \geq 1- \epsilon^2
\end{eqnarray}</p>

<h3 id="a-construction-of-an-adiabatic-search-algorithm">A construction of an adiabatic search algorithm</h3>

<p>Adiabatic thereom can  be appllied to construct a quantum search algorithm.
Consider a problem to find $\vert m \rangle$ out of $N$ computational basis.  Let $\vert \psi_0 \rangle$ be a uniform superposition of computational basis:</p>

<p>\begin{eqnarray}
&amp;&amp; |\psi_0\rangle = \frac{1}{\sqrt{N}}\sum^N_{i=1}|i\rangle
\end{eqnarray}</p>

<p>Define two Hamiltonians $H_0$ and $H_m$ and their time dependent interpolation $H(t)$ as:</p>

<p>\begin{eqnarray}
&amp;&amp; H_0 = I - |\psi_0\rangle\langle\psi_0| \newline
&amp;&amp; H_m = I - |m\rangle\langle m| \newline
&amp;&amp; H(t) = (1-t/T)H_0 + t/T H_m
\end{eqnarray}</p>

<p>Note that $\vert\psi_0\rangle$ and $\vert m\rangle$ are the base states of $H_0$ and $H_m$. To have the adiabatic theorem hold, one need to impose the condition on the only adjustable parameter $T$. First, solve the eigen problem of $H(t)$:</p>

<p>\begin{eqnarray}
&amp;&amp; H(t)|E_k ; t\rangle = E_k |E_k; t\rangle \newline
&amp;&amp; (I-(1-t/T)|\psi_0\rangle\langle \psi_0|-t/T|m\rangle\langle m|)|E_k ; t\rangle = E_k |E_k; t\rangle \newline
\end{eqnarray}</p>

<p>let $a_k=\langle\psi_0\vert E_k;t\rangle$, $b_k=\langle m \vert E_k;t\rangle$, and note that $\langle m \vert \psi_0\rangle = \frac{1}{\sqrt{N}}$, one obtain the following relations by applying the inner product of $\vert m\rangle$ and $\vert\psi_0\rangle$ to the above equation:</p>

<p>\begin{eqnarray}
&amp;&amp; (t/T-E_k)a_k = t/T\frac{1}{\sqrt{N}}b_k \newline
&amp;&amp; (1-t/T-E_k)b_k = (1-t/T)\frac{1}{\sqrt{N}}a_k
\end{eqnarray}</p>

<p>Solving the eigenvalue, with $s = t/T$ one obtain the (degenerate) eigevalue of $H(t)$:</p>

<p>\begin{eqnarray}
&amp;&amp; E_0 = \frac{1}{2}(1-\sqrt{4(1-\frac{1}{N})s^2-4(1-\frac{1}{N})s+1}) \newline
&amp;&amp; E_1 = \frac{1}{2}(1+\sqrt{4(1-\frac{1}{N})s^2-4(1-\frac{1}{N})s+1})
\end{eqnarray}</p>

<p>Then the eigengap $g$ is
\begin{eqnarray}
&amp;&amp; g = \sqrt{1-4\frac{N-1}{N}s(1-s)}
\end{eqnarray}
with minimum reached at $s=1/2$ with $g_{min}=1/\sqrt{N}$.</p>

<p>Following up with the calculation of the variational quantity $\langle\frac{dH}{dt}\rangle_{1,0}$:</p>

<p>\begin{eqnarray}
&amp;&amp; \langle\frac{dH}{dt}\rangle_{1,0} = \langle E_1;t|\frac{1}{T}(H_m - H_0)|E_0;t\rangle \newline
&amp;&amp; = \langle E_1;t|\frac{1}{T}(|\psi_0\rangle\langle \psi_0| - |m\rangle \langle m|)|E_0;t\rangle \newline
\end{eqnarray}</p>

<p>It can be proven that the absolute value of this quantity is less than
$\frac{1}{T}$. Thus,
the adiabatic condition reads:</p>

<p>\begin{eqnarray}
&amp;&amp; T \geq N/\epsilon
\end{eqnarray}</p>

<p>which yields no benefit over classical algorithm.</p>

<h3 id="a-variant-of-the-interpolation-rate-st">A variant of the interpolation rate s(t)</h3>

<p>To achieve quantum speedup, notice that the adiabatic condition can be applied to a infinitesimal scale of time. That is, let $s(t)$ be an unknown function of $t$, and then impose the adiabatic condition:</p>

<p>\begin{eqnarray}
&amp;&amp;\frac{ds}{dt} \leq \epsilon \frac{g^2(t)}{|\langle\frac{dH}{ds}\rangle_{1,0}|}
\end{eqnarray}</p>

<p>Along with previous calculation of the eigengap $g$, one can choose the
following to have the local adiabatic condition hold :
\begin{eqnarray}
&amp;&amp; \frac{ds}{dt} = \epsilon g^2(t)
\end{eqnarray}</p>

<p>Solving $s(t)$ with boundary condition $s(0)=0, s(T)=1$, one obtain 
\begin{eqnarray}
&amp;&amp; T = \frac{\pi}{2\epsilon} \sqrt{N}
\end{eqnarray}
by taking $s = 1$ and $N \gg 1$. This achieve the Grover’s search performance as wanted.</p>

<p>To prove the optimality of the choice $s(t)$, the search problem can be solved if the evolved answers of different target are sufficiently discriminable. That is, let $\vert\psi_m ;t \rangle$ and $\vert\psi_{m’};t\rangle$ be the volved state after time $t$ for search target $m$ and $m’$ respectively, we require that:</p>

<p>\begin{eqnarray}
&amp;&amp; 1- |\langle \psi_{m}; T| \psi_{m’}; T\rangle|^2 \geq \delta \; 
\end{eqnarray}</p>

<p>From Schrodinger’s equation, we have the followings:</p>

<p>\begin{eqnarray}
&amp;&amp; i \frac{d}{dt}|\psi_m ; t\rangle = (I - (1-s)|\psi_0\rangle\langle\psi_0| - s|m\rangle\langle m|)|\psi_m;t\rangle \newline
&amp;&amp; i \frac{d}{dt}|\psi_m’ ; t\rangle = (I - (1-s)|\psi_0\rangle\langle\psi_0| - s|m’\rangle\langle m’|)|\psi_{m’};t\rangle 
\end{eqnarray}</p>

<p>with $\vert \psi_m ; 0 \rangle = \vert \psi_{m} ; 0 \rangle = \vert \psi_0 \rangle$ for some initial configuration. Now take the derivative of the objective with respect to the evolution time :</p>

<p>\begin{eqnarray}
&amp;&amp; \frac{d}{dt}[1 -  |\langle \psi_m ; t| \psi_{m’} ; t \rangle |^2 ] \newline
&amp;&amp; = - \frac{d}{dt}  2 \langle \psi_m ; t| \psi_{m’} ; t \rangle  \langle \psi_{m’} ; t| \psi_{m} ; t \rangle  \newline
&amp;&amp;  = {\rm Im} [\langle \psi_{m} ; t| (-s| m\rangle \langle m| + s |m’ \rangle\langle m’|) | \psi_{m’} ; t\rangle \langle \psi_{m’} ; t| \psi_{m} ; t \rangle]  \newline
&amp;&amp; \leq 2 |\langle \psi_{m} ; t| (-s| m\rangle \langle m| + s |m’ \rangle\langle m’|) | \psi_{m’} ; t\rangle| \; | \langle \psi_{m’} ; t| \psi_{m} ; t \rangle|  \newline
&amp;&amp; \leq  2[ |\langle \psi_{m} ; t| (-s| m\rangle \langle m|) | \psi_{m’} ; t\rangle | + |\langle \psi_{m} ; t| (-s| m’\rangle \langle m’|) | \psi_{m’} ; t\rangle |]<br />
\end{eqnarray}</p>

<p>Let $H_{m} = -s \vert m \rangle \langle m \vert $, summing the above objective for all $m$ and $m’$, we got:</p>

<p>\begin{eqnarray}
&amp;&amp; \frac{d}{dt} \sum_{m, m’ = 1}^{N} [1 -  |\langle \psi_m ; t| \psi_{m’} ; t \rangle |^2 ] \newline
&amp;&amp; \leq 4 \sum_{m, m’ = 1}^{N} |\langle \psi_m ; t| H_{m} | \psi_{m’} ; t \rangle |  \newline
&amp;&amp; \leq 4 \sum_{m, m’ = 1}^{N} \Vert H_{m}| \psi_{m} ; t \rangle \Vert \Vert \psi_{m’} \Vert \newline
&amp;&amp; = 4 N \sum_{m=1}^{N} \Vert H_{m}| \psi_{m} ; t \rangle \Vert<br />
\end{eqnarray}</p>

<p>To upper bound the last term, notice that $\sum_{m=1}^{N} \Vert H_m \vert \psi_m ; t\rangle\Vert^2 = s^2$ due to normalization condition. Thus by Cauchy inequality,</p>

<p>\begin{eqnarray}
&amp;&amp; \frac{d}{dt} \sum_{m, m’ = 1}^{N} [1 -  |\langle \psi_m ; t| \psi_{m’} ; t \rangle |^2 ] \newline
&amp;&amp; \leq 4 N \sum_{m=1}^{N} \Vert H_{m}| \psi_{m} ; t \rangle \Vert \leq 4 N \sqrt{N} s   <br />
\end{eqnarray}</p>

<p>Notice that the above derivative is taken to leverage the Schrodinger’s
equation. Finally, we discard the derivative and integrate the both side of the inequality:</p>

<p>\begin{eqnarray}
&amp;&amp; \sum_{m, m’ = 1}^{N} [1 -  |\langle \psi_m ; t| \psi_{m’} ; t \rangle |^2 ] \leq 4N \sqrt{N} \int_{0}^{T} s(t) dt \leq 4N \sqrt{N} T
\end{eqnarray}</p>

<p>With $\sum_{m, m’ = 1}^{N} [1 -  \vert \langle \psi_m ; t \vert \psi_{m’} ; t \rangle \vert ^2 ] \geq N^2\delta$, we’ve proven that $T \geq O(\frac{\sqrt{N}}{\delta})$.</p>

<h3 id="comment">Comment</h3>

<p>The original work consider the Hamiltonian with a certain form. Perhaps it’s possible to derive search algorithm by other interploations of Hamiltonian. Perhaps a class of Hamiltonian would guarantee speedup.</p>

<p>Also, if take the adiabatic results to the reverse direction, what makes quantum evolution an adiabatic one ? Can similar result be applied to other update dynamic, like recent gradient descent in machine learning ?</p>
]]></content>
  </entry>
  
</feed>
