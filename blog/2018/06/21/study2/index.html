
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="ja"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Briefing : Derivation of the uniform convergence - Tr{Ceng Lou}</title>
  <meta name="author" content="Ping Sheng Kao">

  
  <meta name="description" content="The following notes are summarized from Prof I-Hsiang Wang’s lecture: Mathematic Principles of Machine learning, Lec 3. An alternated derivation is &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://slicerkao.github.io/blog/2018/06/21/study2/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Tr{Ceng Lou}" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$']],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea',
        'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX",
      availableFonts: ["STIX","TeX"] }
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  

</head>

<body >
  <div id="container">
    <header role="banner"><hgroup>
  <h1><a href="/">Tr{Ceng Lou}</a></h1>
  
    <h2>Study Notes</h2>
  
</hgroup>

</header>
    <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="site:slicerkao.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
    <div id="main">
      <div id="content">
        <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title"><a href="">Briefing : Derivation of the Uniform Convergence</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2018-06-21T15:57:36+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


<div class="entry-content"><p>The following notes are summarized from Prof I-Hsiang Wang’s lecture: <a href="http://homepage.ntu.edu.tw/~ihwang/Teaching/Sp18/MPML.html"> Mathematic  Principles  of  Machine  learning</a>, Lec 3. An alternated derivation is also given in the <a href="http://nowak.ece.wisc.edu/SLT09/lecture19.pdf">website note</a> and summarized here.</p>

<!--more-->

<h3 id="erm-and-the-uniform-deviation">ERM and the Uniform deviation</h3>

<p>Given empirical dataset $\mathcal{D}$, ERM leads to a solution $f_{\rm ERM} \in {\rm argmin}_{f \in \mathcal{F}} \hat{L} _{\mathcal{D} _n}(f; P)$. It can be shown that the estimation error $\Xi _{\mathcal{F}} (f _{\rm ERM}) = L(f _{ \rm ERM}; P) - \mathop{\inf} _{f \in \mathcal{F}}L(f;P)$ is upper bounded by the uniform deviation:</p>

<p>\begin{eqnarray}
&amp;&amp; \Xi _{\mathcal{F}}(f _{\rm ERM} ; P) \leq 2 \mathop{\sup} _{f \in \mathcal{F}} |\hat{L} _{\mathcal{D} _n} (f; P) - L(f ; P)|
\end{eqnarray}</p>

<p>Let the r.h.s of the inequality (withdraw the constant 2 and the supremum sign) be denoted by $\Gamma _n (f) = \hat{L} _{\mathcal{D} _n} (f; P) - L(f ; P)$. The goal is then to bound the probability $P[\sup _{f \in \mathcal{F}} \vert \Gamma _n (f) \vert  \geq t]$, which is first upper bounded by the upper and the lower deviation:</p>

<p>\begin{eqnarray}
&amp;&amp; P[\sup _{f \in \mathcal{F}} \vert \Gamma _n (f) \vert  \geq t] \leq P[\sup _{f \in \mathcal{F}} \Gamma _n (f)  \geq t] + P[\sup _{f \in \mathcal{F}} [- \Gamma _n (f)] \geq t] 
\end{eqnarray}</p>

<p>Now that $\sup _{f \in \mathcal{F}}\Gamma _n (f) = \gamma(Z _1, \dots , Z _n) $ is a function of random
variable for training sample $(Z _1, \dots Z _n)$, and the variation of the
function by random variable is bounded above by:</p>

<p>\begin{eqnarray}
&amp;&amp; |\gamma(z _1, z _2, \dots , z _i, \dots , z _n) - \gamma(z _1, z _2, \dots , \tilde{z} _i, \dots , z _n)| \leq \frac{C}{n} 
\end{eqnarray}</p>

<p>for loss value constrained within a interval with length $C$. McDiarmid’s
inequality $P[ f(Z) - E[f(Z)] \geq t ] \leq {\rm exp} (\frac{-2 t^2}{\sum c^2 _k})$ then applies for $c _k = \frac{C}{n}$. Since we consider loss to be constrained within length-1 interval, this yields the following bound :</p>

<p>\begin{eqnarray}
&amp;&amp;  P \, [\sup _{f \in \mathcal{F}} \Gamma _n (f) \geq E[\sup _{f \in \mathcal{F}} \Gamma _n] + t] \leq {\rm exp} (-2nt^2)<br />
\end{eqnarray}</p>

<p>Now we already have a upper bound for the probability of large supremum deviation. What’s left is to upper bound the quantity $E[\sup _{f \in \mathcal{F}} \Gamma _n]$ so that we got:</p>

<p>\begin{eqnarray}
&amp;&amp; P \, [\sup _{f \in \mathcal{F}} \Gamma _n (f) \geq {\rm some \; upper \; bound} + t ] \leq  P \, [\sup _{f \in \mathcal{F}} \Gamma _n (f) \geq E[\sup _{f \in \mathcal{F}} \Gamma _n] + t] \leq {\rm exp} (-2nt^2)<br />
\end{eqnarray}</p>

<h3 id="rademacher-complexity">Rademacher Complexity</h3>

<p>To upper bound the term $E _{D ^n}[\sup _{f \in \mathcal{F}} \Gamma _n]$, note that :
\begin{eqnarray}
&amp;&amp; \mathop{\sup} _{f \in \mathcal{F}} \Gamma _n(f) = \sup _{g \in \ell \circ \mathcal{F}} [\frac{1}{n} \sum _i ^n g(Z _i) - E[g(Z)]]
\end{eqnarray}</p>

<p>Where $\ell$ is the loss function, and $Z$ is adopted in the previous section as $Z = (X,Y)$. Vapnik et.al introduce some ghost sample $ \tilde{D} = (\tilde{Z} _1 \dots \tilde{Z} _n) \sim P^{\otimes n}$ so that $E _{\tilde{D} _n} [\frac{1}{n} \sum _{1}^{n} g(\tilde{Z} _i)] = E(g(Z))$. Introducing this we get:</p>

<p>\begin{eqnarray}
&amp;&amp; E _{D^n}[\mathop{\sup} _{f \in \mathcal{F}} \Gamma _n(f)] = E _{D^n} [\sup _{g \in \ell \circ \mathcal{F}} [\frac{1}{n} \sum _i ^n g(Z _i) - E[g(Z)]]
] \newline
&amp;&amp; = E _{D^n} [\sup _{g \in \ell \circ \mathcal{F}} E _{\tilde{D}^n}
[\frac{1}{n} \sum _i ^n g(Z _i) - g(\tilde{Z} _i)]] \newline
&amp;&amp; \leq E _{D^n} \; E _{\tilde{D}^n} [\sup _{g \in \ell \circ \mathcal{F}}
\frac{1}{n} \sum _i ^n g(Z _i) - g(\tilde{Z} _i)] \newline
\end{eqnarray}
To this end, one can further upper bound it by $E _{D _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} g(Z _i)] + E _{\tilde{D} _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} -g(\tilde{Z} _i)] $, but this leads the analysis to nowhere since the supremum of a same term with different sign has different result with respect to the expectation. To resolve it, introduce a symmetrization random variable (like adding extra term for the summation (integral) expansion expression of the original expectation value) to equalize the value of the two terms. More explicitly, the random variable $g(Z _i) - g(\tilde{Z} _i)$ has a symmetric distribution, thus introducing i.i.d random variables $\sigma _i \sim {\rm Ber} (\frac{1}{2})$ to the expectation won’t change its value. Thus,</p>

<p>\begin{eqnarray}
&amp;&amp; E _{D^n} \; E _{\tilde{D}^n} [\sup _{g \in \ell \circ \mathcal{F}}
\frac{1}{n} \sum _i ^n g(Z _i) - g(\tilde{Z} _i)] \newline
&amp;&amp; = E _{D^n} \; E _{\tilde{D}^n} \; E _{\sigma _1 , \sigma _2 , \dots , \sigma _n } [\sup _{g \in \ell \circ \mathcal{F}}
\frac{1}{n} \sigma _i [\sum _i ^n g(Z _i) - g(\tilde{Z} _i)]] \newline
&amp;&amp; \leq E _{D _n} E _{\sigma _1 \dots \sigma _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(Z _i)] +  E _{\tilde{D} _n} E _{\sigma _1 \dots \sigma _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} - \sigma _i g(\tilde{Z} _i)] \newline
&amp;&amp; = 2 E _{D _n} E _{\sigma _1 \dots \sigma _n} [\mathop{\sup} _{g \in \ell \circ \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(Z _i)] 
\end{eqnarray}</p>

<p>The quantity on the right hand size is called the Rademacher complexity
$\mathcal{R} _n (\mathcal{G}) $ , which is $E _{D _n} E _{\sigma _1 \dots \sigma _n} [\mathop{\sup} _{g \in \mathcal{G}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(Z _i)] $</p>

<p>Before proceeding toward the property of the Rademacher complexity, we can
relate the complexity measure back to the estimation error via :</p>

<p>\begin{eqnarray}
&amp;&amp; P[\Xi _{\mathcal{F}}(f _{\rm ERM} ; P) \geq 4 \mathcal{R} _n(\mathcal{G}) + \epsilon] \leq P[\mathop{\sup} _{f \in \mathcal{F}}|\Gamma _n(f)| &gt; 2 \mathcal{R} _n(\mathcal{G}) + \frac{\epsilon}{2}] \leq 2 {\rm exp} \big( - \frac{n \epsilon^2}{2} \big) 
\end{eqnarray}</p>

<p>Also, with Mcdiamard inequality, one can upper bound the Rademahcer complexity by the empirical Rademacher inequality $\hat{\mathcal{R}} _n (d _n , \mathcal{G}) = E _{\bf \vec{\sigma}} [ \mathop{\sup} _{g \in \mathcal{G}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(z _i)]$ with probability larger than $1-\delta$ :</p>

<p>\begin{eqnarray}
&amp;&amp; \mathcal{R} _n( \mathcal{G}) \leq 2 \hat{\mathcal{R}} _n (d _n , \mathcal{G}) + 3 \sqrt{\frac{2}{n} \log \big(\frac{3}{\delta}\big)}<br />
\end{eqnarray}</p>

<h3 id="binary-classification-0-1-loss-and-the-vc-dimension">Binary Classification, 0-1 loss and the VC dimension</h3>

<p>The empirical Rademacher complexity can be further related to the functional
class $\mathcal{F}$ in the binary classification setting, with $0-1$ loss:</p>

<p>\begin{eqnarray}
&amp;&amp; \hat{\mathcal{R}} _n (d _n; \mathcal{G}) = E _{\bf \vec{\sigma}} [ \mathop{\sup} _{g \in \mathcal{G}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i g(z _i)] \newline
&amp;&amp; = E _{\bf \vec{\sigma}} [ \mathop{\sup} _{f \in \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i \ell (f(x _i, y _i))] \newline
&amp;&amp; = E _{\bf \vec{\sigma}} [ \mathop{\sup} _{f \in \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i \frac{1 - y _i f(x _i)}{2}] \newline
&amp;&amp; = \frac{1}{2}E _{\bf \vec{\sigma}} [ \mathop{\sup} _{f \in \mathcal{F}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i f(x _i)] = \frac{1}{2} \hat{R} _n(x _1, \dots x _n; \mathcal{F})
\end{eqnarray}</p>

<p>The above expression has a good convenience: We can simplify the supremum term into maximum of a finite functional class, since the original functional class can be partitioned into groups $\mathcal{A}$ that evaluate the same on the empirical data set $(x _1 \dots x _n)$. Hence,</p>

<p>\begin{eqnarray}
&amp;&amp; \hat{R} _n(x _1, \dots x _n; \mathcal{F}) = E _{\bf \sigma} \big[ \mathop{max} _{u \in \mathcal{A}} \frac{1}{n} \sum _{i = 1}^{n} \sigma _i u _i \big]
\end{eqnarray}</p>

<p>The size of the set $\mathcal{A}$ is upper bounded by the growth function $\tau_{\mathcal{F}}(n)$, which is defined as:</p>

<p>\begin{eqnarray}
&amp;&amp; \tau _{\mathcal{F}} (n) = \mathop{max} _{(x _1 \dots x _n)} \tau _{\mathcal{F}} (x _1 \dots x _n)= \mathop{max} _{(x _1 \dots x _n)} \big\vert [(f(x _1) \dots f(x _n)) | f \in \mathcal{F}] \big\vert
\end{eqnarray}</p>

<p>With this one can adopt the Massart’s finite Lemma : If $\mathcal{A}$ is a
finite set, then :</p>

<p>\begin{eqnarray}
&amp;&amp; E _{\bf \sigma} \big[ \mathop{max} _{u \in \mathcal{A}} \frac{1}{n} \sum _{i=1}^{n} \sigma _i u _i  \big] \leq \frac{\rm max _{u \in \mathcal{A}} \Vert u \Vert}{n} \sqrt{2 \log |\mathcal{A}|}
\end{eqnarray}</p>

<p>(which is proven by maximum of subgaussian). Direct application of this lemma leads to :
\begin{eqnarray}
&amp;&amp; \hat{\mathcal{R}} _n (\bf x = (x _1 \dots x _n) ; \mathcal{F}) \leq \sqrt{\frac{2 \log \tau _{\mathcal{F}}(\bf x)}{n}}, \; \; \; \mathcal{R} _n(\mathcal{F}) \leq \sqrt{\frac{2 \log \tau _{\mathcal{F}} (n)}{n}}
\end{eqnarray}</p>

<p>Now we can introduce the VC dimension, which is defined as:
\begin{eqnarray}
&amp;&amp; VC(\mathcal{F}) = {\rm max}[n : \tau _{\mathcal{F}} (n) = 2^n]
\end{eqnarray}</p>

<p>Rethink about the meaning of the growth function $\tau _{\mathcal{F}} (n)$ in the case $n$ greater or less than the VC dimension $d$ of the functional class, we could arrive at the Sauer’s lemma:</p>

<p>\begin{eqnarray}
&amp;&amp; \tau _{F} (n) \leq \sum _{i =0}^{ \mathop{min} (n, d)} {n \choose i} \longrightarrow ({\rm which \; is \; further \; upper \; bounded \; by \;}  (n + 1) ^d)
\end{eqnarray}</p>

<p>Hence, for 0-1 loss binary classification task, we have with probability at
least $1- \delta$:
\begin{eqnarray}
&amp;&amp; \Xi _{\mathcal{F}}(f _{ERM} ; P) \leq 2 \sqrt{\frac{2 VC(\mathcal{F}) \log(n+1)}{n}} + \sqrt{\frac{2}{n} \log (\frac{2}{\delta})}
\end{eqnarray}</p>

<h3 id="relaxation-to-non-0-1-loss-and-non-binary-case">Relaxation to non 0-1 loss and non binary case</h3>
<p>From the above analysis, it’s assumed that the label cardinality $\vert
\mathcal{Y} \vert$ is finite so that $\vert \mathcal{A} \vert \leq \vert \mathcal{Y} \vert ^n$ is finite and the Massart finite lemma apply. Also, it’s assumed that the loss function is hard $0-1$, and thus the functional class can be perfectly grouped. It may not be the case when the loss function is real valued.</p>

<p>Having Massart’s lemma in mind, we still want to discretize the functional
class. What can be done is to introduce the covering number. Define a pseudo
distance $\rho _{z _1 \dots z _n} (f,g) = \sqrt{\frac{1}{n} \sum _{i=1}^n |f(z _i) - g(z _i)|^2}$, it satisfies triangle inequality and symmetry property. But it’s not a metric since indistinguishability is not guaranteed : $\rho( f, g) = 0$ does not imply $ f = g$. An $\epsilon -cover$ of a functional class $\mathcal{G} \subset \mathcal{U}$ (($\mathcal{U}$, $\rho$) is a pseudometric space) is defined as a subset $\mathcal{C} \subset \mathcal{U}$ such that for all $g \in \mathcal{G}$, there exist $h \in \mathcal{C}$ that $\rho (g, h) \leq \epsilon$. The covering number is then defined as:</p>

<p>\begin{eqnarray}
&amp;&amp; N(\epsilon, \mathcal{G}, \rho) = {\rm min}[|mathcal{C}|: \mathcal{C} \; is \; an \; \epsilon -cover \; of \; \mathcal{G}]
\end{eqnarray}</p>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Ping Sheng Kao</span></span>

      








  



<time datetime="2018-06-21T15:57:36+08:00" pubdate data-updated="true"></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/machinelearning/'>machinelearning</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://slicerkao.github.io/blog/2018/06/21/study2/" data-via="" data-counturl="http://slicerkao.github.io/blog/2018/06/21/study2/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2018/06/19/study-note-2/" title="Previous Post: Briefing : PAC learnibility">&laquo; Briefing : PAC learnibility</a>
      
      
        <a class="basic-alignment right" href="/blog/2018/07/11/study3/" title="Next Post: Study Note: Understanding Generative Adversarial Net (1)">Study Note: Understanding Generative Adversarial Net (1) &raquo;</a>
      
    </p>
  </footer>
</article>


</div>

      </div><!-- /div#content -->
    </div><!-- /div#main -->
  </div><!-- /div.container -->
  <footer><div id="footer-widgets-wrapper">
  <div id="footer-first" class="footer-widget">
    <h3>About Me</h3>
    <section class="about-me">
      
      <div>
        <ul>
          
            <li>GitHub: <a href="https://github.com/SlicerKao">@SlicerKao</a></li>
          
          
            <li>Blog: <a href="http://slicerkao.github.io">http://slicerkao.github.io</a></li>
        </ul>
        <p>
            Machine to learn a quantum leap ?
        </p>
      </div>
    </section>
  </div><!-- /div#footer-second -->


  <div id="footer-second" class="footer-widget">
    <h3>Recent Posts</h3>
    <section id="hatena-popular" class="hatena-bookmark">
      <script language="javascript" type="text/javascript" src="http://b.hatena.ne.jp/js/widget.js" charset="utf-8"></script>
      <script language="javascript" type="text/javascript">
        Hatena.BookmarkWidget.url   = "http://slicerkao.github.io";
        Hatena.BookmarkWidget.title = "Recent Posts";
        Hatena.BookmarkWidget.sort  = "hot";
        Hatena.BookmarkWidget.width = 0;
        Hatena.BookmarkWidget.num   = 10;
        Hatena.BookmarkWidget.theme = "notheme";
        Hatena.BookmarkWidget.load();
      </script>
    </section>
  </div><!-- /div#footer-second -->

  <div id="footer-third" class="footer-widget">
    <h3>Popular Posts</h3>
    <section id="hatena-popular" class="hatena-bookmark">
      <script language="javascript" type="text/javascript" src="http://b.hatena.ne.jp/js/widget.js" charset="utf-8"></script>
      <script language="javascript" type="text/javascript">
        Hatena.BookmarkWidget.url   = "http://slicerkao.github.io";
        Hatena.BookmarkWidget.title = "Popular Posts";
        Hatena.BookmarkWidget.sort  = "count";
        Hatena.BookmarkWidget.width = 0;
        Hatena.BookmarkWidget.num   = 10;
        Hatena.BookmarkWidget.theme = "notheme";
        Hatena.BookmarkWidget.load();
      </script>
    </section>
  </div><!-- /div#footer-third -->
</div><!-- /div#footer-widgets-wrapper -->

<div id="credit" role="contentinfo">
  <p>
    Copyright &copy; 2018 - <a href="https://github.com/Ping Sheng Kao/">Ping Sheng Kao</a> -
    <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/haya14busa/mjolvim-octotheme">Mjolvim</a></span>
  </p>
</div>

</footer>
  










  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']]
    }
  });
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
