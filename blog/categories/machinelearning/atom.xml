<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machinelearning | Tr{Ceng Lou}]]></title>
  <link href="http://slicerkao.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://slicerkao.github.io/"/>
  <updated>2018-07-18T12:32:55+08:00</updated>
  <id>http://slicerkao.github.io/</id>
  <author>
    <name><![CDATA[Ping Sheng Kao]]></name>
    <email><![CDATA[samk840125@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Understanding Generative Adversarial Net (1)]]></title>
    <link href="http://slicerkao.github.io/blog/2018/07/11/study3/"/>
    <updated>2018-07-11T21:25:48+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/07/11/study3</id>
    <content type="html"><![CDATA[<p>Recently a bunch of Generative Adversarial Net (GAN) related research has boomed in the unsupervised learning community, and it seems interesting to fully understand the mechanism of this novel learning framework. Just a few days ago (July 5th, 2018), Alexia Jolicoeur-Martineau et al proposed a <a href="https://arxiv.org/abs/1807.00734">Relativistic GAN</a> that aimed to provide improvement via changes of the discriminator term in the original GAN objective function. It pointed out that the real data are usually ignored during the learning process in most GAN paradigm, and thus it proposed a relativistic quantity to measure how ‘real’ the real samples are compared to the fake sample.</p>

<p><img src="/images/de.png" alt="alt text" /></p>

<!--more-->

<p>It seems to to me that the original goal of GAN is to train the followings given the sampled real data sets $x_i \mathop{\sim}^{i.i.d} P$:</p>

<ul>
  <li>
    <p>A generator $G_{\theta}(z)$ that has small distributional divergence with respect to the true distribution. This is the main goal of the generative learning paradigm.</p>
  </li>
  <li>
    <p>A discriminator $D_{\omega}$ equipped with strong detection ability. This is rather a side product for GAN, but the training procedure relies on the detection result (e.g. the backpropagation for weight update.)</p>
  </li>
</ul>

<h3 id="generator-perspective-minimization-of-the-divergence">Generator perspective: minimization of the divergence</h3>

<p>For the first part, most GAN objective functions are some $f$ divergence with the following forms:</p>

<p>\begin{eqnarray}
&amp;&amp; D_f(P||Q) = \int_{\chi} q(x)f \Big( \frac{p(x)}{q(x)}\Big) dx
\end{eqnarray}</p>

<p>As pointed out in the <a href="https://arxiv.org/abs/1606.00709">$f$ GAN literature</a>, the $f$ divergence is approximated via a variational lower bound known as Frenchel conjugate $f^\ast$ (for all lower-semicontious function $f$). This conjugate is convex with the defintion:</p>

<p>\begin{eqnarray}
&amp;&amp; f^\ast (t) = \mathop{\sup}_{u \in {\rm dom}_f} [ut - f(u)]
\end{eqnarray}</p>

<p>Likewise, $f$ can be expressed in terms of $f^\ast$ since the pair $(f, f^\ast)$ are dual to each other. Plugging into the expression of the $f$ divergence, we have:</p>

<p>\begin{eqnarray}
&amp;&amp; D_f(P||Q) = \int_{\chi} q(x) \mathop{\sup}_{t \in {\rm
dom} _{f^\ast}} \Big( t\frac{p(x)}{q(x)} - f^\ast(t) \Big) dx \newline 
&amp;&amp; \geq \sup _{T(x) \in \mathcal{T}} \Big( \int _{\chi} p(x) T(x) dx - \int _{\chi} q(x) f^\ast (T(x)) \Big) \newline
&amp;&amp; = \sup _{T(x) \in \mathcal{T}} (E _{x \sim P}[T(x)] - E _{x \sim Q}[f^ \ast (T(x))])
\end{eqnarray}</p>

<p>To achieve the tightest lower bound, $T(x) = \frac{df}{dx} \big(\frac{p(x)}{q(x)}\big)$. For instance, consider a $f$ divergence with $f(u) = u \log u - (u+1) \log (u+1)$, first compute the Frenchel dual as :</p>

<p>\begin{eqnarray}
&amp;&amp; f^\ast (t) = \mathop{\sup} _{u &gt; 0} [ut - u \log u + (u+1) \log(u+1)] \newline
&amp;&amp; = - (t + \log (10^{-t} - 1)) \; , \; t &lt; 0 
\end{eqnarray}</p>

<p>So the variational lower bound for the $f$ divergence becomes :</p>

<p>\begin{eqnarray}
&amp;&amp; \sup _{T(x) \in \mathcal{T}} (E _{x \sim P}[T(x)] + E _{x \sim Q}[\log (1 - exp(T(x)))])
\end{eqnarray}</p>

<p>Standard GAN adopts $T(x)$ with the form :</p>

<p>\begin{eqnarray}
&amp;&amp; T(x) = \log D_\omega(x) = 1/(1 + e^{-V_{\omega}(x)})
\end{eqnarray}</p>

<p>So the loss is the familiar form :</p>

<p>\begin{eqnarray}
&amp;&amp; L(\theta, \omega) = \mathop{\sup} _{D} [E _{x \sim P}[\log D _{\omega}(x)] + E _{x \sim G _{\theta}(P_z)}[\log (1 - D _{\omega}(x))]]
\end{eqnarray}</p>

<p>When $D(x)$ is optimal, it has $D(x) = \frac{p(x)}{p(x) + G_{\theta} (P_z)(x)}$, or with $Q_{\theta} = G_{\theta}(P_z)$ as a shorthand, $D(x) = \frac{p(x)}{p(x) + q_{\theta}(x)}$. Plugging it back to the variational bound, one can recover the J-S divergence in the literature. Thus with a perfect discriminator, the generator can be trained to minimize the divergence loss between generated distribution and the target distribution.</p>

<h3 id="discriminator-perspective-optimal-detector">Discriminator perspective: Optimal Detector</h3>

<p>The above arguments stand on a generator perspective, where the goal is to
minmize a divergence measure (with high fidelity). On the other hand, we’ve seen that the optimal discriminator to construct JS divergence is actually a maximum likelihood detector. ML is also an optimal detector for hypthothesis testing with uniform prior, where the performance metric is the Bayesian risk. Such a correspondence between divergence measure and the optimal Bayesian detector is not a coincidence: The form of the standard GAN variational lower bound takes a log-likelihood expression. i.e, when jointly maximize the log of true alarm and negative probability, one get a J-S divergence measure with a Bayesian optimal detector.</p>

<p>For more example, consider a variational objective with $0-1$ loss, i.e.,</p>

<p>\begin{eqnarray}
&amp;&amp; L(\theta, \omega) = \mathop{\sup} _{D : \chi \to 0, 1} [ E _{x \sim P}[\mathbb{1}[D _{\omega}(x)]] + 
E _{G _{\theta}(z) \sim Q}[1 - \mathbb{1}[D _{\omega}(G _{\theta}(x))]]]
\end{eqnarray}</p>

<p>This loss is exactly (up to a 1/2 prior probability) related to the Bayesian risk, and the maximization of it leads to a divergence measure called total variance $d_{\rm TV}(P,Q)$, defined as:</p>

<p>\begin{eqnarray}
&amp;&amp; d_{\rm TV}(P,Q) = \mathop{\sup}_{S \subset \chi} [P(S) - Q(S)] = L(\theta, \omega^\ast) - 1
\end{eqnarray}</p>

<p>So to construct an optimal detector in a sense of the Baysian risk, the discriminator should jointly maximize the true positive and negative probability. And its maximum is $1 + d _{\rm TV}(P,Q)$. Likewise, standard GAN jointly maximize the log likelihood of true positive and negative probabilty, which yields an optimal value expressed by the J-S divergence. For other GAN, one can also trace the decision region of the optimal discriminator, and may understand if such a GAN bias toward true positive or true negative probability.</p>

<h3 id="dose-gan-bias-toward-true-positive-or-true-negative-during-the-training-process-">Dose GAN bias toward true positive or true negative during the training process ?</h3>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Briefing : Derivation of the uniform convergence]]></title>
    <link href="http://slicerkao.github.io/blog/2018/06/21/study2/"/>
    <updated>2018-06-21T15:57:36+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/06/21/study2</id>
    <content type="html"><![CDATA[<p>The following notes are summarized from Prof I-Hsiang Wang’s lecture: <a href="http://homepage.ntu.edu.tw/~ihwang/Teaching/Sp18/MPML.html"> Mathematic  Principles  of  Machine  learning</a>, Lec 3 for</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Briefing : PAC learnibility]]></title>
    <link href="http://slicerkao.github.io/blog/2018/06/19/study-note-2/"/>
    <updated>2018-06-19T16:47:04+08:00</updated>
    <id>http://slicerkao.github.io/blog/2018/06/19/study-note-2</id>
    <content type="html"><![CDATA[<p>The following notes are summarized from Prof I-Hsiang Wang’s lecture: <a href="http://homepage.ntu.edu.tw/~ihwang/Teaching/Sp18/MPML.html"> Mathematic  Principles  of  Machine  learning</a>, covering lecture 1 and 2.</p>

<!--more-->

<h3 id="discriminative-learning-consistency-and-no-free-lunch">Discriminative learning, Consistency and No free lunch</h3>

<p>In the setting of discriminative learning, <em>excessive risk</em> $R_{exessive}$ is decomposed into estimation error (which is associated with sampling bias) and the approximation error (which is associated with the size of the functional class $\mathcal{F}$) as :</p>

<p>$R_{excessive}  = L(f, P)- L(f^* ,P) = (L(f, P) - \mathop{\inf}_{f_n \in \mathcal{F}} L(f_n, P)) \longrightarrow \; {\rm estimation \; error}$</p>

<p>$+ \; ( \mathop{\inf}_{f_n \in \mathcal{f}} L(f_n, p) - L(f^* , p))
\longrightarrow \; {\rm approximation \; error}$</p>

<p>where $f^{\ast}$  denotes the Baysian optimal rule. The term <em>consistency</em> for a sequence of learning algorithms $f_n $ and a distribution $\mathcal{P}$ is defined as:
\begin{eqnarray}
&amp;&amp; L(f_n ; P) - L(f^* ; P) \mathop{\rightarrow}^{L_1}0 \; as \; n \rightarrow \infty
\end{eqnarray}</p>

<p>And with loss bounded from above, by dominate convergence theorem the
consistency criteria is equivalent to convergence in probability:
\begin{eqnarray}
\forall \, \epsilon &gt; 0, \; \mathop{\lim}_{n \leftarrow \infty} P^{\otimes n}[|L(f_n; P)- L(f^*; P)| &lt; \epsilon ] = 1 
\end{eqnarray}</p>

<p>Impossibility result holds for general learning paradigms as well as
discriminative paradigm. No free lunch theorem states the existence of bad
distribution such that small excessive risk can’t be achieved with finite
samples. Therefore discriminative approach confine the functional group to circumvent bad convergent behaviour. This derives the concept of PAC learnibility: There’re some functional class being learnbable, and some that are not.</p>

<p>Note that the lower bound for the imposibility results is established via a key observation: If a
distribution $\mathcal{P}$ is concentrated on a finite set of data $X$ with $P(Y|X)$ being deterministic , then the algorithm output can be highly biased. It’s due to the fact that the training data may not cover all of the element in $X$, which make the algorithm only conduct random guess for the unseen data. (i.e. $f(X_1,Y_1, X_2, Y_2 \dots X_n, Y_n) \perp Y_{\rm test} $)</p>

<h3 id="pac-learnable-realizability">PAC learnable, realizability</h3>

<p>In the discriminative paradigm we aim to minimize the estimation error $\Xi_{\mathcal{F}}(L,P) = L(f,P) -
\mathop{\inf}_{f_n \in \mathcal{F}}L(f_n, P)$, and we’re interested in the
number of samples required to achieve certain ($\epsilon, \delta $)
generalization guarantee. Define the sample complexity of a hypothesis class
$\mathcal{F}$ as</p>

<p>\begin{eqnarray}
&amp;&amp; n_{\mathcal{F}}(\epsilon, \delta) \mathop{=}^{\Delta} {\rm min}[n: \, \exists f_n \in \mathcal{F} \, {\rm s.t.} \, \forall P, \, P^{\otimes n}[\Xi_{\mathcal{F}}(f_n, P) \leq \epsilon] \geq 1-\delta]
\end{eqnarray}</p>

<p>So if there’s some learning algorithm $f_n \in \mathcal{F}$ that guarantee estimation error to be less than
$\epsilon$ with high confident level and finite sample for all distribution,
than such a functional class in PAC learnable.</p>

<p>ERM rule provide a sufficient condition of PAC learnability, via 
\begin{eqnarray}
&amp;&amp; \Xi_{\mathcal{F}}(f_{ERM},P) \leq 2 \mathop{\sup}_{f\in \mathcal{F}}|\Delta
(f,P)|
\end{eqnarray}</p>

<p>Where $\Delta (f, P)$ is the deviation between true risk and the empirical risk.</p>

<p><em>Realizability</em> indicates a existance of $f \in \mathcal{F}$ such that L(f,P) is
zero, which is equivalent to $\ell (f(X), Y) = 0$ with probability one. In this
case:</p>

<ul>
  <li>
    <p>ERM always achieve zero emprirical risk.</p>
  </li>
  <li>
    <p>$f^{*}$ is a deterministic fuction</p>
  </li>
</ul>

<p>Thus any <em>bad</em> function $f_{\rm bad} \in \mathcal{F}$ is less probable to be
picked out from ERM rule. This indeed provided a sample commplexity with smaller
scaling factor $\mathcal{O}(\epsilon^{-1})$, as compared with the agnostic case
$\mathcal{O}(\epsilon^{-2})$</p>

]]></content>
  </entry>
  
</feed>
